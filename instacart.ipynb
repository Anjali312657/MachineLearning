{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "# Useful function for printing rules out in readable format\n",
    "products = pd.read_csv(os.path.join('..', 'resource', 'asnlib', 'publicdata', 'products.csv.bz2'))\n",
    "product_table = dict(products[['product_id', 'product_name']].to_dict('tight')['data'])\n",
    "\n",
    "def print_rules(rules):\n",
    "    for LHS, RHS in rules:\n",
    "        if isinstance(LHS, int):\n",
    "            l_string = f'{LHS}'\n",
    "        else:\n",
    "            l_string = f'{\", \".join([product_table[p_id] for p_id in LHS])}'\n",
    "        if isinstance(RHS, int):\n",
    "            r_string = f'{RHS}'\n",
    "        else:\n",
    "            r_string = f'{\", \".join([product_table[p_id] for p_id in RHS])}'\n",
    "        print(f'{l_string} -> {r_string}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def map1(key, value, itemsets):\n",
    "    # Emit every product from every order\n",
    "    for product in value:\n",
    "        yield product, 1\n",
    "\n",
    "def reduce1(key, value):\n",
    "    # Emit every product that appears more than once\n",
    "    count = sum(value)\n",
    "    if count > 5:\n",
    "        yield (key,), count\n",
    "\n",
    "def map2(key, value, itemsets):\n",
    "    # Emit every pair of products in this order where both products are in the frequent itemsets\n",
    "    for pair in itertools.combinations([product for product in sorted(value) if (product,) in itemsets], 2):\n",
    "        yield pair, 1\n",
    "\n",
    "def reduce2(key, value):\n",
    "    # Emit every pair of products that appears more than once\n",
    "    count = sum(value)\n",
    "    if count > 5:\n",
    "        yield key, count\n",
    "\n",
    "# Define the sequence of functions to be run by the MapReduce \"platform\"\n",
    "stages = [map1, reduce1, map2, reduce2]\n",
    "\n",
    "# Transform frequent itemsets into association rules\n",
    "\n",
    "def itemsets2rules(itemsets):\n",
    "    rules = []\n",
    "    for itemset, value in itemsets.items():\n",
    "        if len(itemset) > 1:\n",
    "            # Puts last item on RHS, everything else on LHS\n",
    "            item_list = list(itemset)\n",
    "            rules.append((set(item_list[:-1]), set(item_list[-1:])))\n",
    "    return rules\n",
    "\n",
    "def prune_rules(rules, max_length=2):\n",
    "    pruned_rules = [rule for rule in rules if len(rule[0]) + len(rule[1]) <= max_length]\n",
    "    \n",
    "    return pruned_rules\n",
    "\n",
    "def precision_rules(itemsets):\n",
    "    rules = itemsets2rules(itemsets)\n",
    "    pruned_rules = prune_rules(rules)\n",
    "    \n",
    "    return pruned_rules\n",
    "\n",
    "def recall_rules(itemsets):\n",
    "    rules = itemsets2rules(itemsets)\n",
    "    pruned_rules = prune_rules(rules)\n",
    "    \n",
    "    return pruned_rules\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "MapReduce",
     "locked": true,
     "points": "5",
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 99,260 orders\n",
      "Keys output by mapper: 35,098 [steps: 1,000,001]\n",
      "Map 1 took 3 seconds\n",
      "Frequent itemsets: 15,357 [steps: 1,015,358]\n",
      "Reduce 1 took 0 seconds\n",
      "Keys output by mapper: 3,477,858 [steps: 7,771,545]\n",
      "Map 2 took 13 seconds\n",
      "Frequent itemsets: 153,959 [steps: 7,910,147]\n",
      "Reduce 2 took 2 seconds\n",
      "Rules created (precision): 138,602\n",
      "Rules created (recall): 138,602\n",
      "MapReduce total time=19 s, steps=7,910,147\n"
     ]
    }
   ],
   "source": [
    "# Reads in the data file to be used to extract the association rules\n",
    "import time\n",
    "\n",
    "product_orders = pd.read_csv(os.path.join('..', 'resource', 'asnlib', 'publicdata', 'order_products__prior.csv.bz2'),\n",
    "                             nrows=1000001)\n",
    "\n",
    "baskets = product_orders.groupby(['order_id'])['product_id']\n",
    "itemsets = {}\n",
    "print(f'Using {len(baskets):,} orders')\n",
    "\n",
    "assert len(stages) % 2 == 0, 'There should be an even number of stages (i.e., matching pairs of map and reduce)'\n",
    "\n",
    "# Serialized MapReduce\n",
    "map_phase = True\n",
    "steps = 0\n",
    "start_whole = time.time()\n",
    "for stage_num, worker_fun in enumerate(stages):\n",
    "    start = time.time()\n",
    "    if map_phase:\n",
    "        map_out = {}\n",
    "        for order_id, products in baskets:\n",
    "            for key, value in worker_fun(order_id, list(products), itemsets):\n",
    "                # Accumulate list of values for each key\n",
    "                try:\n",
    "                    map_out[key].append(value)\n",
    "                except KeyError:\n",
    "                    map_out[key] = [value]\n",
    "                steps += 1\n",
    "        print(f'Keys output by mapper: {len(map_out):,} [steps: {steps:,}]')\n",
    "    else:\n",
    "        for key, value in map_out.items():\n",
    "            for new_key, new_value in worker_fun(key, value):\n",
    "                itemsets[new_key] = new_value\n",
    "                steps += 1\n",
    "        print(f'Frequent itemsets: {len(itemsets):,} [steps: {steps:,}]')\n",
    "    stage_time = round(time.time()-start)\n",
    "    print(f'{\"Map\" if map_phase else \"Reduce\"} {stage_num//2+1} took {stage_time:d} seconds')\n",
    "    map_phase = not map_phase\n",
    "# Generate rules\n",
    "rules_p = precision_rules(itemsets)\n",
    "rules_r = recall_rules(itemsets)\n",
    "duration = int(round(time.time()-start_whole))\n",
    "print(f'Rules created (precision): {len(rules_p):,}')\n",
    "print(f'Rules created (recall): {len(rules_r):,}')\n",
    "print(f'MapReduce total time={duration:,} s, steps={steps:,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Rule Extraction",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 938 orders, spanning 4,660 products\n",
      "Rules fired 1,996,118 times\n",
      "Precision-oriented rules have a TP rate of 5.24/order, and a FP rate of 1198.12/order\n",
      "Rules fired 1,996,118 times\n",
      "Recall-oriented rules have a TP rate of 5.24/order, and a FP rate of 1198.12/order\n"
     ]
    }
   ],
   "source": [
    "def check_rules(rules):\n",
    "    \"\"\"Make sure rules are well-formed\"\"\"\n",
    "    for i, rule in enumerate(rules):\n",
    "        assert len(rule) == 2, f'Rule {rule} is not a list/tuple of length 2 (i.e., LHS and RHS)'\n",
    "        LHS, RHS = rule\n",
    "        if isinstance(LHS, int):\n",
    "            LHS = set(LHS)\n",
    "        if isinstance(RHS, int):\n",
    "            RHS = set(RHS)\n",
    "        rules[i] = (set(LHS), set(RHS))\n",
    "        overlap = rules[i][0] & rules[i][1]\n",
    "        assert len(overlap) == 0, f'Overlapping LHS and RHS: {\", \".join(sorted(overlap))}'\n",
    "    \n",
    "def evaluate_rules(rules, baskets):\n",
    "    rule_firings = 0\n",
    "    tp = fp = 0\n",
    "    for order, product_series in baskets:\n",
    "        # Do any rules match?\n",
    "        product_set = set(product_series)\n",
    "        predictions = set()\n",
    "        for LHS, RHS in rules:\n",
    "            is_fired = len(product_set & LHS) == len(LHS)\n",
    "            if is_fired:\n",
    "                # Rule fires\n",
    "                rule_firings += 1\n",
    "                predictions |= RHS\n",
    "        # Predicted items that appear in order\n",
    "        tp += len(predictions & product_set)\n",
    "        # Predicted items that do not appear in order\n",
    "        fp += len(predictions - product_set)\n",
    "    print(f'Rules fired {rule_firings:,} times')\n",
    "    return tp/len(baskets), fp/len(baskets)\n",
    "\n",
    "check_rules(rules_p)\n",
    "check_rules(rules_r)\n",
    "# Test rules on a separate data set\n",
    "test_orders = pd.read_csv(os.path.join('..', 'resource', 'asnlib', 'publicdata', 'order_products__train.csv.bz2'),\n",
    "                          nrows=9998)\n",
    "baskets = test_orders.groupby(['order_id'])['product_id']\n",
    "products = set(test_orders['product_id'].unique())\n",
    "print(f'Testing {len(baskets):,} orders, spanning {len(products):,} products')\n",
    "\n",
    "tp_rate_p, fp_rate_p = evaluate_rules(rules_p, baskets) \n",
    "print(f'Precision-oriented rules have a TP rate of {tp_rate_p:.02f}/order, and a FP rate of {fp_rate_p:.02f}/order')\n",
    "tp_rate_r, fp_rate_r = evaluate_rules(rules_r, baskets) \n",
    "print(f'Recall-oriented rules have a TP rate of {tp_rate_r:.02f}/order, and a FP rate of {fp_rate_r:.02f}/order')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
